# Описание реализованных заданий лабораторной работы №1

## Задание 1. Оценка числа π методом Монте-Карло (`distributed_computing_lab_1/src/monte_carlo_pi.c`)
- **Идея:** случайные точки равномерно распределяются в квадрате \([-1, 1] \times [-1, 1]\). Доля точек, попавших в единичную окружность, приближается к \(\pi/4\).
- **Параллельная схема:** каждый процесс генерирует собственный поднабор точек, используя детерминированный LCG со смещением по рангу.
- **Коллективные операции:** `MPI_Reduce` суммирует количество попаданий; главный процесс вычисляет приближение π и выводит статистику времени.
- **Особенности:** объём работы распределяется как `total_samples / comm_sz` c корректной раздачей остатка; общая синхронизация по `MPI_Barrier` до и после вычислений для замера времени.

## Задание 2. Умножение матрицы на вектор

### 2.1. Разбиение по строкам (`distributed_computing_lab_1/src/matvec_rows.c`)
- **Данные:** матрица заранее формируется на ранге 0 (детерминированные значения). Вектор также генерируется на корневом процессе.
- **Распределение:** `MPI_Scatterv` отправляет процессам последовательные блоки строк; каждому процессу достаётся `row_counts[rank]` строк, что позволяет обрабатывать случаи, когда число строк не делится на число процессов.
- **Локальная работа:** каждое ядро перемножает свой поднабор строк с общим вектором.
- **Сбор результата:** `MPI_Gatherv` возвращает частичные суммы на ранг 0. Корневой процесс выполняет проверку корректности последовательным перемножением.

### 2.2. Разбиение по столбцам (`distributed_computing_lab_1/src/matvec_cols.c`)
- **Проблема:** стандартная раздача столбцов требует пересортировки данных, чтобы каждому процессу достились целые столбцы.
- **Решение:** корневой процесс упаковывает матрицу в столбцовый формат (функция `PackColumns`) и передаёт блоки через `MPI_Scatterv`.
- **Вычисления:** каждый процесс хранит часть столбцов матрицы и соответствующие компоненты вектора; локально вычисляется частичный вклад в результирующий вектор.
- **Агрегация:** `MPI_Reduce` суммирует частичные результаты по всем процессам.
- **Проверка:** на ранге 0 проводится последовательное перемножение для валидации.

### 2.3. Блочное разбиение (`distributed_computing_lab_1/src/matvec_blocks.c`)
- **Сетка процессов:** число процессов должно образовывать квадрат (например, 4, 9, 16). Создаётся логическая решётка \(q \times q\).
- **Разбиение:** матрица делится на блоки по строкам и столбцам; каждому процессу передаётся блок `row_counts[row] × col_counts[col]`. Вектор нарезается на блоки столбцов.
- **Коммуникации:** блочная матрица рассылается из корневого процесса при помощи `MPI_Send`/`MPI_Recv` (для простоты). Компоненты вектора раздаются аналогично.
- **Вычисления:** локально считается произведение блоковой строки матрицы на соответствующий блок вектора.
- **Редукция:** процессы, находящиеся в одной строке решётки, объединяются через `MPI_Comm_split`; внутри строк выполняется `MPI_Reduce`, чтобы сложить вклады блоков. Далее блоки результирующего вектора собираются на ранге 0.
- **Проверка:** глобальный результат проверяется на корневом процессе.

## Задание 3. Алгоритм Кэннона (`distributed_computing_lab_1/src/matmul_cannon.c`)
- **Требования:** квадратная решётка процессов \(q \times q\); размер матрицы кратен \(q\).
- **Инициальный сдвиг:** перед основной фазой каждая матрица выполняет циклический сдвиг — блоки матрицы A сдвигаются влево на `row` позиций, матрицы B — вверх на `col` позиций. Реализовано через `MPI_Sendrecv_replace`.
- **Главный цикл:** \(q\) стадий. На каждой стадии выполняется локальное умножение блоков размером `block_dim × block_dim`, результат аккумулируется в локальной матрице C.
- **Коммуникации:** после каждой стадии блоки A и B циклически пересылаются соседям по кольцу (влево и вверх соответственно).
- **Сбор:** после завершения стадий блоки C собираются на нулевом процессе с помощью парного `MPI_Send`/`MPI_Recv`.
- **Валидация:** корневой процесс перемножает исходные матрицы последовательно и сверяет результат.

## Используемые коллективные операции и паттерны
- `MPI_Bcast` — распространение параметров и вектора.
- `MPI_Scatterv` / `MPI_Gatherv` — неравномерное распределение и сбор строк матрицы.
- `MPI_Reduce` — агрегирование частичных результатов (π, столбцы, блоки).
- `MPI_Comm_split` и декартова топология `MPI_Cart_create` — логическая группировка процессов.
- `MPI_Sendrecv_replace` — циклические сдвиги в алгоритме Кэннона.
